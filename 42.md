## Benchmark One Stop Page

This page should answer the most important questions that you may have w.r.t. to Elasticsearch benchmarks. It is kept in a private repo because we also talk about some internals here.

### Which types of benchmarks are available?

* **Nightly benchmarks** of latest Elasticsearch master are publicly available at [https://elasticsearch-benchmarks.elastic.co/](https://elasticsearch-benchmarks.elastic.co/).
* **Release benchmarks** are run for different configuration of Elasticsearch and our [Elasticsearch Docker image](https://github.com/elastic/elasticsearch-docker). The results are also published at [https://elasticsearch-benchmarks.elastic.co/](https://elasticsearch-benchmarks.elastic.co/).
* **Long-running benchmarks** of the latest Elasticsearch master snapshot build. It uses a [logging](https://github.com/elastic/rally-internal-tracks/tree/master/logs) benchmark for 30 days on three nodes. It is more of a stability test than a benchmark and only available internally. The [results are published for Elastic employees](https://elasticsearch-benchmarks-internal.elastic.co/app/kibana#/dashboard/Long-Running-Benchmarks).

The two former benchmarks are triggered via [Jenkins](https://elasticsearch-ci.elastic.co/view/All/job/elastic+elasticsearch+master+macrobenchmark-periodic/) and run on bare-metal machines (see our [benchmarking methodology](https://elasticsearch-benchmarks.elastic.co/) on details about the machines).

Our long-running benchmarks also run on bare-metal machines but are just triggered directly on the machines as it does not make too much sense to leave a Jenkins build running for one month.

### How are the benchmarks run? Can I also execute them?

All macrobenchmarks are run with [Rally](https://github.com/elastic/rally) which is open source. Rally also has a [user documentation](http://esrally.readthedocs.io/en/latest/).

Our benchmark suite is maintained in a separate open source repo, called [rally-tracks](https://github.com/elastic/rally-tracks). The Elasticsearch configurations that we support out of the box for our benchmarks are maintained at [rally-teams](https://github.com/elastic/rally-teams). Again, all this is open source.

We also document our [benchmarking methodology](https://elasticsearch-benchmarks.elastic.co/) publicly.

### Where are the benchmark machines?

The nightly benchmarks run on the following machines:

| public DNS name                                            | role                                              | private IP (10 Gbit) |
|------------------------------------------------------------|---------------------------------------------------|----------------------|
| ``slave-746202.build.hetzner-fsn1-dc4.elasticnet.co``      | Jenkins slave, benchmark coordinator, load driver | 192.168.14.2         |
| ``target-746203.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node                             | 192.168.14.3         |
| ``target-746204.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node                             | 192.168.14.4         |
| ``target-746205.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node                             | 192.168.14.5         |

The long running benchmarks run on the following machines:

| public DNS name                                             | role                               | private IP (10 Gbit) |
|-------------------------------------------------------------|------------------------------------|----------------------|
| ``longrun-669376.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark coordinator, load driver | 192.168.14.76        |
| ``longrun-669377.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.77        |
| ``longrun-669378.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.78        |
| ``longrun-669380.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.80        |

We also have a dedicated environment for adhoc benchmarks:

| public DNS name                                            | role                               | private IP (10 Gbit) |
|------------------------------------------------------------|------------------------------------|----------------------|
| ``memory-869742.benchmark.fsn1-dc4.hetzner.elasticnet.co`` | benchmark coordinator, load driver | 192.168.14.12        |
| ``memory-869743.benchmark.fsn1-dc4.hetzner.elasticnet.co`` | benchmark target node              | 192.168.14.13        |

All ten machines (nightly, long-running and adhoc) are located in the same rack and connected via a dedicated 10 GBit switch.

For details about the machine hardware and software configuration, please see our [benchmarking methodology](https://elasticsearch-benchmarks.elastic.co/).

All benchmarks run as Unix user ``jenkins`` so if you want to inspect anything, issue ``sudo -iu jenkins`` after logging in.

### What benchmark suites are available?

* The official benchmark suite of Rally, called [rally-tracks](https://github.com/elastic/rally-tracks) which is open-source.
* A company-internal benchmark suite used for various purposes, called [rally-internal-tracks](https://github.com/elastic/rally-internal-tracks).
* [rally-eventdata-track](https://github.com/elastic/rally-eventdata-track) to simulate simulating event-based data use-cases. It also includes a simulation of the usage patterns of Kibana dashboards.

### What do we do to ensure reproducible results?

1. We only run on bare-metal. The load test driver is physically separated from the benchmark candidate server and all machines are in the same rack and connected via a high throughput network connection.
2. The machines are specially setup to be as silent as possible. If you are interested, check out the [respective playbook](https://github.com/elastic/infra/blob/master/ansible/playbooks/macrobenchmarks_targets.yml) and the associated roles in the infra repo.
3. System software (kernel, JDK) is only updated very seldom and we document each and every upgrade publicly.
4. Rally captures lots of details about the system it runs on so we know for each measurement sample that we store: the exact Elasticsearch version (git hash), JDK version, Rally version and kernel version.
5. We run a set of fixtures before each benchmark run: At least we [drop the page cache and SLAB objects](https://github.com/elastic/night-rally/tree/master/fixtures/ansible/roles/drop-caches) and [TRIM the disk](https://github.com/elastic/night-rally/tree/master/fixtures/ansible/roles/trim).

### A new release came out a few days ago. Where are the results?

Each trial run with our benchmark suite takes several hours to run (roughly half a day) and at the moment we run the following configurations:

* Bare metal benchmarks on an unencrypted drive
* Bare metal benchmarks on an encrypted drive (encryption at rest)
* Benchmarks of our Elasticsearch Docker image on an unencrypted drive

Apart from that we also use the same hardware for our nightlies to ensure one can actually compare the results between nightlies and official releases. As a consequence, this takes several days in total.

### What types of benchmarks are not available?

We usually only run regular benchmark that are mainly focused on the needs of the development team. As such we do not have benchmarks for:

* Very specific system configurations (different file systems, I/O schedulers, ...)
* Very specific configurations of Elasticsearch
* Specific combinations of plugins
* Windows benchmarks
* Benchmarks how Elasticsearch behaves on different types of EC2 instances or instances at other cloud providers

We also do not have any scalability benchmarks (yet). Christian Dahlqvist does run [sizing benchmarks with Rally](https://github.com/elastic/sizing-benchmarks) though. Please also see the [list of open issues](https://github.com/elastic/night-rally/issues) to get see what's on the roadmap.

If you are missing a certain benchmark, please go ahead, use Rally and run it. All the tooling is available as open-source and it is well-documented. However, if you think we should run a specific benchmark regularly, please [raise an issue](https://github.com/elastic/night-rally/issues/new).

### Where is the web page for elasticsearch-benchmarks.elastic.co stored?

It is hosted from the S3 bucket `elasticsearch-benchmarks.elastic.co` in the AWS Elasticsearch account, US East (N. Virginia) region.

### My question is not answered

Please [raise an issue](https://github.com/elastic/night-rally/issues/new).
