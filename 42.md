## Benchmark One Stop Page

This page should answer the most important questions that you may have w.r.t. to Elasticsearch benchmarks. It is kept in a private repo because we also talk about some internals here.

### Which types of benchmarks are available?

* **Nightly benchmarks** of latest Elasticsearch master are publicly available at [https://elasticsearch-benchmarks.elastic.co/](https://elasticsearch-benchmarks.elastic.co/).
* **Release benchmarks** are run for different configuration of Elasticsearch and our [Elasticsearch Docker image](https://github.com/elastic/elasticsearch/tree/master/distribution/docker). The results are also published at [https://elasticsearch-benchmarks.elastic.co/](https://elasticsearch-benchmarks.elastic.co/).
* **Long-running benchmarks** of the latest Elasticsearch master snapshot build. It uses the [rally-eventdata-track](https://github.com/elastic/rally-eventdata-track) benchmark for 30 days on three nodes. It is more of a stability test than a benchmark and only available internally. The [results are published for Elastic employees](https://elasticsearch-benchmarks-internal.elastic.co/app/kibana#/dashboard/bd85b180-8a5c-11e8-8558-f33069e7a81e).

The two former utilize two groups of machines in parallel, due to the total time required to execute all benchmarks, and are triggered by corresponding Jenkins Jobs: [group-1](https://elasticsearch-ci.elastic.co/view/All/job/elastic+elasticsearch+master+macrobenchmark-periodic-group-1/), [group-2](https://elasticsearch-ci.elastic.co/view/All/job/elastic+elasticsearch+master+macrobenchmark-periodic-group-2/).

Our long-running benchmarks also run on bare-metal machines but are just triggered directly on the machines as it does not make too much sense to leave a Jenkins build running for one month. Results from long-running benchmarks can be found in this [Kibana dashboard](https://ela.st/elasticsearch-long-running-benchmarks).

### How are the benchmarks run? Can I also execute them?

All macrobenchmarks are run with [Rally](https://github.com/elastic/rally) which is open source. Rally also has a [user documentation](http://esrally.readthedocs.io/en/latest/).

Our benchmark suite is maintained in a separate open source repo, called [rally-tracks](https://github.com/elastic/rally-tracks). The Elasticsearch configurations that we support out of the box for our benchmarks are maintained at [rally-teams](https://github.com/elastic/rally-teams). Again, all this is open source.

We also document our [benchmarking methodology](https://elasticsearch-benchmarks.elastic.co/) publicly.

### Where are the benchmark machines?

**Note**: This is mentioned only for informational purposes. The definitive reference is our [ssh configuration](https://github.com/elastic/night-rally/tree/master/external/ssh_config).

The nightly benchmarks run on the following machines:

nightly-group-1:

| public DNS name                                            | role                                              | private IP (10 Gbit) |
|------------------------------------------------------------|---------------------------------------------------|----------------------|
| ``worker-953730.build.fsn1-dc14.hetzner.elasticnet.co``    | Jenkins worker, benchmark coordinator, load driver| 192.168.20.30        |
| ``target-953729.benchmark.fsn1-dc14.hetzner.elasticnet.co``| benchmark target node                             | 192.168.20.29        |
| ``target-953732.benchmark.fsn1-dc14.hetzner.elasticnet.co``| benchmark target node                             | 192.168.20.32        |
| ``target-953733.benchmark.fsn1-dc14.hetzner.elasticnet.co``| benchmark target node                             | 192.168.20.33        |

nightly-group-2:

| public DNS name                                            | role                                              | private IP (10 Gbit) |
|------------------------------------------------------------|---------------------------------------------------|----------------------|
| ``worker-953731.build.fsn1-dc14.hetzner.elasticnet.co``    | Jenkins worker, benchmark coordinator, load driver| 192.168.20.31        |
| ``target-953734.benchmark.fsn1-dc14.hetzner.elasticnet.co``| benchmark target node                             | 192.168.20.34        |
| ``target-953735.benchmark.fsn1-dc14.hetzner.elasticnet.co``| benchmark target node                             | 192.168.20.35        |
| ``target-953736.benchmark.fsn1-dc14.hetzner.elasticnet.co``| benchmark target node                             | 192.168.20.36        |

The nightly benchmarks have been split into two groups to ensure all benchmarks finish well below 24 hours. All machines are located in the same rack and connected via a dedicated 10 GBit switch.

The long running benchmarks run on the following machines:

| public DNS name                                             | role                               | private IP (10 Gbit) |
|-------------------------------------------------------------|------------------------------------|----------------------|
| ``longrun-669376.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark coordinator, load driver | 192.168.14.76        |
| ``longrun-669377.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.77        |
| ``longrun-669378.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.78        |
| ``longrun-669380.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.80        |

We also have two dedicated environments for adhoc benchmarks:

The "low-memory" environment

| public DNS name                                            | role                               | private IP (10 Gbit) |
|------------------------------------------------------------|------------------------------------|----------------------|
| ``memory-869742.benchmark.fsn1-dc4.hetzner.elasticnet.co`` | benchmark coordinator, load driver | 192.168.14.12        |
| ``memory-869743.benchmark.fsn1-dc4.hetzner.elasticnet.co`` | benchmark target node              | 192.168.14.13        |

The previous nightly environment:

| public DNS name                                            | role                               | private IP (10 Gbit) |
|------------------------------------------------------------|------------------------------------|----------------------|
| ``slave-746202.build.hetzner-fsn1-dc4.elasticnet.co``      | benchmark coordinator, load driver | 192.168.14.2         |
| ``target-746203.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.3         |
| ``target-746204.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.4         |
| ``target-746205.benchmark.hetzner-fsn1-dc4.elasticnet.co`` | benchmark target node              | 192.168.14.5         |


All ten machines (long-running, low-memory and the previous nightly environment) are located in the same rack and connected via a dedicated 10 GBit switch.

For details about the machine hardware and software configuration, please see our [benchmarking methodology](https://elasticsearch-benchmarks.elastic.co/).

All benchmarks run as Unix user ``jenkins`` so if you want to inspect anything, issue ``sudo -iu jenkins`` after logging in.

### What benchmark suites are available?

* The official benchmark suite of Rally, called [rally-tracks](https://github.com/elastic/rally-tracks) which is open-source.
* A company-internal benchmark suite used for various purposes, called [rally-internal-tracks](https://github.com/elastic/rally-internal-tracks).
* [rally-eventdata-track](https://github.com/elastic/rally-eventdata-track) to simulate simulating event-based data use-cases. It also includes a simulation of the usage patterns of Kibana dashboards.

### What do we do to ensure reproducible results?

1. We only run on bare-metal. The load test driver is physically separated from the benchmark candidate server and all machines are in the same rack and connected via a high throughput network connection.
2. The machines are specially setup to be as silent as possible. If you are interested, check out the [respective playbook](https://github.com/elastic/infra/blob/master/ansible/playbooks/macrobenchmarks_targets.yml) and the associated roles in the infra repo.
3. System software (kernel, JDK) is only updated very seldom and we document each and every upgrade publicly.
4. Rally captures lots of details about the system it runs on so we know for each measurement sample that we store: the exact Elasticsearch version (git hash), JDK version, Rally version and kernel version.
5. We run a set of fixtures before each benchmark run: At least we [drop the page cache and SLAB objects](https://github.com/elastic/night-rally/tree/master/night_rally/fixtures/ansible/roles/drop-caches) and [TRIM the disk](https://github.com/elastic/night-rally/tree/master/night_rally/fixtures/ansible/roles/trim).

### A new release came out a few days ago. Where are the results?

Each trial run with our benchmark suite takes several hours to run (roughly half a day) and at the moment we run the following configurations:

* Bare metal benchmarks on an unencrypted drive
* Bare metal benchmarks on an encrypted drive (encryption at rest)
* Benchmarks of our Elasticsearch Docker image on an unencrypted drive

Apart from that we also use the same hardware for our nightlies to ensure one can actually compare the results between nightlies and official releases. As a consequence, this takes several days in total.

### What types of benchmarks are not available?

We usually only run regular benchmark that are mainly focused on the needs of the development team. As such we do not have benchmarks for:

* Very specific system configurations (different file systems, I/O schedulers, ...)
* Very specific configurations of Elasticsearch
* Specific combinations of plugins
* Windows benchmarks
* Benchmarks how Elasticsearch behaves on different types of EC2 instances or instances at other cloud providers

We also do not have any scalability benchmarks (yet). Please also see the [list of open issues](https://github.com/elastic/night-rally/issues) to see what's on the roadmap.

If you are missing a certain benchmark, please go ahead, use Rally and run it. All the tooling is available as open-source and it is well-documented. However, if you think we should run a specific benchmark regularly, please [raise an issue](https://github.com/elastic/night-rally/issues/new).

### Where is the web page for elasticsearch-benchmarks.elastic.co stored?

It is stored in the S3 bucket `elasticsearch-benchmarks.elastic.co` in the AWS Elasticsearch account, US East (N. Virginia) region and hosted via a [Fastly proxy](https://github.com/elastic/infra/issues/10695).

### My question is not answered

Please [raise an issue](https://github.com/elastic/night-rally/issues/new).
